<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="UTF-8">
  <meta 
    name="viewport"
    content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta 
    http-equiv="X-UA-Compatible" 
    content="ie=edge">
  <meta 
    name="theme-color" 
    content="#fff" 
    id="theme-color">
  <meta 
    name="description" 
    content="Lynn&#39;s Blog">
  <link 
    rel="icon" 
    href="/en/img/logo.png">
  <title>[Cluster API] Deploy cluster on Vsphere</title>
  
    
      <meta 
        property="og:title" 
        content="[Cluster API] Deploy cluster on Vsphere">
    
    
      <meta 
        property="og:url" 
        content="http://lzreload.com/en/2022/09/22/Deploy-cluster-on-Vsphere-using-cluster-api/index.html">
    
    
      <meta 
        property="og:img" 
        content="/img/author.png">
    
    
    
      <meta 
        property="og:type" 
        content="article">
      <meta 
        property="og:article:published_time" 
        content="2022-09-22">
      <meta 
        property="og:article:modified_time" 
        content="2022-09-22">
      <meta 
        property="og:article:author" 
        content="lz-nsc">
      
        
          <meta 
            property="og:article:tag" 
            content="Kubernetes">
        
          <meta 
            property="og:article:tag" 
            content="Cluster API">
        
      
    
  
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
    function loadCSS(href, data, attr) {
      var sheet = document.createElement('link');
      sheet.ref = 'stylesheet';
      sheet.href = href;
      sheet.dataset[data] = attr;
      document.head.appendChild(sheet);
    }
    function changeCSS(cssFile, data, attr) {
      var oldlink = document.querySelector(data);
      var newlink = document.createElement("link");
      newlink.setAttribute("rel", "stylesheet");
      newlink.setAttribute("href", cssFile);
      newlink.dataset.prism = attr;
      document.head.replaceChild(newlink, oldlink);
    }
  </script>
  
    
      
      
      
      
        
        
        
        <script>
          function prismThemeChange() {
            if(document.getElementById('theme-color').dataset.mode === 'dark') {
              if(document.querySelector('[data-prism]')) {
                changeCSS('/en/js/lib/prism/prism-tomorrow.min.css', '[data-prism]', 'prism-tomorrow');
              } else {
                loadCSS('/en/js/lib/prism/prism-tomorrow.min.css', 'prism', 'prism-tomorrow');
              }
            } else {
              if(document.querySelector('[data-prism]')) {
                changeCSS('/en/js/lib/prism/prism.min.css', '[data-prism]', 'prism');
              } else {
                loadCSS('/en/js/lib/prism/prism.min.css', 'prism', 'prism');
              }
            }
          }
          prismThemeChange()
        </script>
      
      
        
        <link rel="stylesheet" href="/en/js/lib/prism/prism-line-numbers.min.css">
      
    
  
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
  </script>
  
    <script>
      var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
        document.getElementById('theme-color').dataset.mode = setting;
        prismThemeChange();
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
        document.getElementById('theme-color').dataset.mode = setting;
        prismThemeChange();
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
        document.getElementById('theme-color').dataset.mode = getCssMediaQuery();
        prismThemeChange();
      }
    };
    setDarkmode();
    </script>
  
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  
  
    <link rel="preload" href="/en/js/lib/lightbox/baguetteBox.min.js" as="script">
    <link rel="preload" href="/en/js/lib/lightbox/baguetteBox.min.css" as="style" >
  
  
    <link rel="preload" href="/en/js/lib/lozad.min.js" as="script">
  
  
  
  
  
    
    <link rel="prefetch" href="//unpkg.com/valine/dist/Valine.min.js" as="script">
  
  
  
  <link rel="stylesheet" href="/en/css/main.css">
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css">
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">
  
    <link rel="stylesheet" href="/en/js/lib/lightbox/baguetteBox.min.css">
  
<meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/en/atom.xml" title="Lynn's Blog" type="application/atom+xml">
</head>

  <body>
    <div class="wrapper">
       
      <nav class="navbar">
  <div class="navbar-logo">
    <span class="navbar-logo-main">
      
        <img 
          class="navbar-logo-img"
          width="32"
          height="32"
          src="/en/img/logo.png" 
          alt="blog logo">
      
      <span class="navbar-logo-dsc">Lynn's blog</span>
    </span>
  </div>
  <div class="navbar-menu">
    
      <a 
        href="/en/" 
        class="navbar-menu-item">
        
          Home
        
      </a>
    
      <a 
        href="/en/archives" 
        class="navbar-menu-item">
        
          Archive
        
      </a>
    
      <a 
        href="/en/tags" 
        class="navbar-menu-item">
        
          Tags
        
      </a>
    
      <a 
        href="/en/categories" 
        class="navbar-menu-item">
        
          Categories
        
      </a>
    
      <a 
        href="/en/about" 
        class="navbar-menu-item">
        
          About
        
      </a>
    
      <a 
        href="/en/links" 
        class="navbar-menu-item">
        
          Friends
        
      </a>
    
      <a 
        href="http://lzreload.com/" 
        class="navbar-menu-item">
        
          中文
        
      </a>
    
    <a 
      class="navbar-menu-item darknavbar" 
      id="dark">
      <i class="iconfont icon-weather"></i>
    </a>
    <a 
      class="navbar-menu-item searchnavbar" 
      id="search">
      <i 
        class="iconfont icon-search" 
        style="font-size: 1.2rem; font-weight: 400;">
      </i>
    </a>
  </div>
</nav> 
      
      <div 
        id="local-search" 
        style="display: none">
        <input
          class="navbar-menu-item"
          id="search-input"
          placeholder="请输入搜索内容..." />
        <div id="search-content"></div>
      </div>
      
      <div class="section-wrap">
        <div class="container">
          <div class="columns">
            <main class="main-column">
<article class="card card-content">
  <header>
    <h1 class="post-title">
      [Cluster API] Deploy cluster on Vsphere
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2022-09-22T11:48:59.000Z">
      <i 
        class="iconfont icon-calendar" 
        style="margin-right: 2px;">
      </i>
      <span>2022-09-22</span>
    </time>
    
    
      <span class="dot"></span>
      <span>5.7k words</span>
    
  </div>
  
    <div 
      class="post-meta post-show-meta" 
      style="margin-top: -10px;">
      <div style="display: flex; align-items: center;">
        <i 
          class="iconfont icon-biaoqian" 
          style="margin-right: 2px; font-size: 1.15rem;">
        </i>
        
          
          <a 
            href="/en/tags/Kubernetes/" 
            class="post-meta-link">
            Kubernetes
          </a>
        
          
            <span class="dot"></span>
          
          <a 
            href="/en/tags/Cluster-API/" 
            class="post-meta-link">
            Cluster API
          </a>
        
      </div>
    </div>
  
  </header>
  <div 
    id="section" 
    class="post-content">
    <h1 id="Environment-requirements"><a href="#Environment-requirements" class="headerlink" title="Environment requirements"></a>Environment requirements</h1><p>Make sure these tools are all installed:</p>
<ul>
<li>kubectl</li>
<li>go</li>
<li>docker</li>
<li>clusterctl</li>
<li>kind</li>
</ul>
<h1 id="Deployment-Steps"><a href="#Deployment-Steps" class="headerlink" title="Deployment Steps"></a>Deployment Steps</h1><h2 id="1-Configure-clusterctl"><a href="#1-Configure-clusterctl" class="headerlink" title="1. Configure clusterctl"></a>1. Configure clusterctl</h2><p>Create a file named <code>clusterctl.yaml</code> in folder <code>.cluster-api</code>, which is normally located at <code>$Home</code>.</p>
<p>For example:</p>
<pre class="line-numbers language-none"><code class="language-none">## -- Controller settings -- ##
VSPHERE_USERNAME: &quot;...&quot; # The username used to access the remote vSphere endpoint
VSPHERE_PASSWORD: &quot;...&quot; # The password used to access the remote vSphere endpoint

## -- Required workload cluster default settings -- ##
VSPHERE_SERVER: &lt;vcenter_server_ip&gt; # The vCenter server IP or FQDN
VSPHERE_DATACENTER: &lt;vcenter_data_center&gt; # The vSphere datacenter to deploy the management cluster on
VSPHERE_DATASTORE: &lt;datastore_name&gt; # The vSphere datastore to deploy the management cluster on
VSPHERE_NETWORK: &lt;network_name&gt; # The VM network to deploy the management cluster on
VSPHERE_RESOURCE_POOL: &lt;Path_to_resource_pool&gt; # The vSphere resource pool for your VMs( About how to get correct path of VSPHERE_RESOURCE_POOL, Please check the Troubleshooting #1 section.)
VSPHERE_FOLDER: &quot;&quot; # The VM folder for your VMs. Set to &quot;&quot; to use the root vSphere folder
VSPHERE_TEMPLATE: &lt;template_name&gt; # The VM template to use for your management cluster.
CONTROL_PLANE_ENDPOINT_IP: &lt;control_plane_ip&gt;&quot; # the IP that kube-vip is going to use as a control plane endpoint
VSPHERE_TLS_THUMBPRINT: &quot;...&quot;  # sha1 thumbprint of the vcenter certificate, can be gotten with this command: openssl x509 -sha1 -fingerprint -in ca.crt -noout
EXP_CLUSTER_RESOURCE_SET: &quot;true&quot; # This enables the ClusterResourceSet feature that we are using for deploying CSI
VSPHERE_SSH_AUTHORIZED_KEY: &quot;ssh-rsa ...&quot; # The public ssh authorized key on all machines, Set it to &quot;&quot; if you don&#39;t want to use ssh to access to nodes
VSPHERE_STORAGE_POLICY: &quot;&quot; # This is the vSphere storage policy. Set it to &quot;&quot; if you don&#39;t want to use a storage policy.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>Remember to set these before we init the management cluster, otherwise, we might get <code>an error about the cluster resource set</code>( If this error occurs, we need to edit the deployment of capi-controller-manager as shown below with <code>kubectl</code>)</p>
<pre class="line-numbers language-none"><code class="language-none">#deploy&#x2F;capi-controller-manager
spec:
      containers:
      - args:
        - --leader-elect
        - --metrics-bind-addr&#x3D;localhost:8080
        - --feature-gates&#x3D;MachinePool&#x3D;false,ClusterResourceSet&#x3D;true,ClusterTopology&#x3D;false<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>Set <code>ClusterResourceSet</code> to <code>true</code> like what is shown in the code above.</p>
<h2 id="2-Use-Kind-and-Cluserctl-to-create-a-management-cluster"><a href="#2-Use-Kind-and-Cluserctl-to-create-a-management-cluster" class="headerlink" title="2. Use Kind and Cluserctl to create a management cluster"></a>2. Use Kind and Cluserctl to create a management cluster</h2><p>(1) Use <code>Kind</code> to create a new Kubernetes cluster (A docker container actually).</p>
<p>(2) Configure <code>kubeconfig</code>, make sure the current context of <code>kubectl</code> is pointing to the <code>Kind</code> cluster.</p>
<p>(3) Use <code>clusterctl</code> to transform the current cluster to a management cluster (remember to specify using <code>vsphere</code> as infrastructure provider).</p>
<pre class="line-numbers language-none"><code class="language-none"># Create kind cluster
$ kind create cluster

# Set current context, the default name of this kind cluster will be kind-kind
$ kubectl cluster-info --context kind-kind

# Transform current cluster to management cluster
$ clusterctl init --infrastructure vsphere<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>In this step, <code>clusterctl</code> will install 4 components into the cluster: <code>cluster-api</code>, <code>bootstrap-kubeadm</code>, <code>control-plan-kubeadm</code> and <code>infrastructure-vsphere</code>. Then we can use <code>kubectl get pods --all-namespaces</code> command to check the status of the management cluster. If some pods are stuck at ImagePullBackOff status, please check Troubleshooting #3 section.</p>
<h3 id="3-Download-the-ova-file-and-deploy-a-template-on-Vsphere"><a href="#3-Download-the-ova-file-and-deploy-a-template-on-Vsphere" class="headerlink" title="3. Download the ova file and deploy a template on Vsphere"></a>3. Download the ova file and deploy a template on Vsphere</h3><p>It is required that machines provisioned by CAPV have <code>cloud-init</code>, <code>kubeadm</code>, and a <code>container runtime</code> pre-installed. We can use one of the CAPV machine images generated by SIG Cluster Lifecycle as a VM template. Images are available here: <a target="_blank" rel="noopener" href="https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/blob/master/README.md#kubernetes-versions-with-published-ovas">ova download</a>.</p>
<h2 id="4-Deploy-workload-cluster"><a href="#4-Deploy-workload-cluster" class="headerlink" title="4. Deploy workload cluster."></a>4. Deploy workload cluster.</h2><p>(1) Generate YAML file of the cluster:</p>
<p>Use the command shown below to generate a YAML file of the target cluster.</p>
<pre class="line-numbers language-none"><code class="language-none">clusterctl generate cluster &lt;cluster name&gt; --kubernetes-version &lt;kubernete version&gt; --control-plane-machine-count 1  --worker-machine-count 3 &gt; cluster.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>(2) Deploy target cluster</p>
<p>Deploy cluster with command <code>kubectl apply -f cluster.yaml</code>.</p>
<p>After these two steps, we can get info about the cluster with the command <code>kubectl get cluster</code> command:</p>
<pre class="line-numbers language-none"><code class="language-none">$ kubectl get cluster
NAME     PHASE         AGE   VERSION
eighth   Provisioned   23h<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>If nothing goes wrong (no situation like wrong Vsphere configuration[Troubleshooting #1] or missing kube-vip image[Troubleshooting #2]), then all VMs (master node and workers) will be created on Vsphere.</p>
<p>With <code>kubectl get machine</code> command, we can check the status of VMs that have been created for our cluster.</p>
<pre class="line-numbers language-none"><code class="language-none">$ kubectl get machine
NAME                           CLUSTER   NODENAME                       PROVIDERID                                       PHASE     AGE   VERSION
eighth-md-0-7b848c46d8-69m2n   eighth    eighth-md-0-7b848c46d8-69m2n   vsphere:&#x2F;&#x2F;423aff2a-386c-f7e1-ca4a-3d1fcde217c6   Running   23h   v1.20.1
eighth-md-0-7b848c46d8-7m6h5   eighth    eighth-md-0-7b848c46d8-7m6h5   vsphere:&#x2F;&#x2F;423a2464-d3ee-4c75-96ac-3202609cc2ee   Running   23h   v1.20.1
eighth-md-0-7b848c46d8-cbnwf   eighth    eighth-md-0-7b848c46d8-cbnwf   vsphere:&#x2F;&#x2F;423a1791-3d80-1f8f-a071-b7b5900f69ea   Running   23h   v1.20.1
eighth-tf4mg                   eighth    eighth-tf4mg                   vsphere:&#x2F;&#x2F;423af9cd-4b1e-617c-a778-b4e435c4efd0   Running   23h   v1.20.1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>And now we can generate a <code>kubeconfig</code>(with <code>clusterctl get kubeconfig &lt;cluster_name&gt;</code> command) to access this new cluster:</p>
<pre class="line-numbers language-none"><code class="language-none">$ clusterctl get kubeconfig eighth &gt; cluster.kubeconfig

$ KUBECONFIG&#x3D;cluster.kubeconfig kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                      READY   STATUS              RESTARTS   AGE     IP               NODE                           NOMINATED NODE   READINESS GATES
kube-system   coredns-74ff55c5b-f9qp7                   0&#x2F;1     Pending             0          3m57s   &lt;none&gt;           &lt;none&gt;                         &lt;none&gt;           &lt;none&gt;
kube-system   coredns-74ff55c5b-qdm4m                   0&#x2F;1     Pending             0          3m57s   &lt;none&gt;           &lt;none&gt;                         &lt;none&gt;           &lt;none&gt;
kube-system   etcd-eighth-tf4mg                         1&#x2F;1     Running             0          3m50s   10.103.226.233   eighth-tf4mg                   &lt;none&gt;           &lt;none&gt;
kube-system   kube-apiserver-eighth-tf4mg               1&#x2F;1     Running             0          3m50s   10.103.226.233   eighth-tf4mg                   &lt;none&gt;           &lt;none&gt;
kube-system   kube-controller-manager-eighth-tf4mg      1&#x2F;1     Running             0          3m50s   10.103.226.233   eighth-tf4mg                   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-2njsn                          1&#x2F;1     Running             0          3m57s   10.103.226.233   eighth-tf4mg                   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-5cw4z                          1&#x2F;1     Running             0          22s     10.103.226.235   eighth-md-0-7b848c46d8-69m2n   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-5kknb                          1&#x2F;1     Running             0          7s      10.103.226.236   eighth-md-0-7b848c46d8-7m6h5   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-mmtzw                          1&#x2F;1     Running             0          78s     10.103.226.234   eighth-md-0-7b848c46d8-cbnwf   &lt;none&gt;           &lt;none&gt;
kube-system   kube-scheduler-eighth-tf4mg               1&#x2F;1     Running             0          3m50s   10.103.226.233   eighth-tf4mg                   &lt;none&gt;           &lt;none&gt;
kube-system   kube-vip-eighth-tf4mg                     1&#x2F;1     Running             0          3m49s   10.103.226.233   eighth-tf4mg                   &lt;none&gt;           &lt;none&gt;
kube-system   vsphere-cloud-controller-manager-2ldmr    0&#x2F;1     ImagePullBackOff    0          3m58s   10.103.226.233   eighth-tf4mg                   &lt;none&gt;           &lt;none&gt;
kube-system   vsphere-cloud-controller-manager-5fp4w    0&#x2F;1     ImagePullBackOff    0          23s     10.103.226.235   eighth-md-0-7b848c46d8-69m2n   &lt;none&gt;           &lt;none&gt;
kube-system   vsphere-cloud-controller-manager-6pxj7    0&#x2F;1     ContainerCreating   0          7s      10.103.226.236   eighth-md-0-7b848c46d8-7m6h5   &lt;none&gt;           &lt;none&gt;
kube-system   vsphere-cloud-controller-manager-nlg8r    0&#x2F;1     ImagePullBackOff    0          78s     10.103.226.234   eighth-md-0-7b848c46d8-cbnwf   &lt;none&gt;           &lt;none&gt;
kube-system   vsphere-csi-controller-5456544dd5-htkvn   0&#x2F;5     Pending             0          3m59s   &lt;none&gt;           &lt;none&gt;                         &lt;none&gt;           &lt;none&gt;
kube-system   vsphere-csi-node-g4wgk                    0&#x2F;3     ContainerCreating   0          78s     &lt;none&gt;           eighth-md-0-7b848c46d8-cbnwf   &lt;none&gt;           &lt;none&gt;
kube-system   vsphere-csi-node-lnt7t                    0&#x2F;3     ContainerCreating   0          3m59s   &lt;none&gt;           eighth-tf4mg                   &lt;none&gt;           &lt;none&gt;
kube-system   vsphere-csi-node-sm2fg                    0&#x2F;3     ContainerCreating   0          23s     &lt;none&gt;           eighth-md-0-7b848c46d8-69m2n   &lt;none&gt;           &lt;none&gt;
kube-system   vsphere-csi-node-xcs5d                    0&#x2F;3     ContainerCreating   0          7s      &lt;none&gt;           eighth-md-0-7b848c46d8-7m6h5   &lt;none&gt;           &lt;none&gt;
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<hr>
<p>According to the output, there are several problems that need to be solved.</p>
<p>If we get <code>ImagePullBackOff</code>, the Troubleshooting #3  section might be helpful.</p>
<p>Also,<code>coredns</code> pods will be stuck at <code>Pending</code> because CNI is missing in this cluster and we need to deploy it manually.</p>
<h2 id="5-Deploy-CNI"><a href="#5-Deploy-CNI" class="headerlink" title="5. Deploy CNI"></a>5. Deploy CNI</h2><p><code>Calico</code> is used in my cluster. We can use this command to deploy it to our cluster:</p>
<pre class="line-numbers language-none"><code class="language-none">#Deploy CNI
$kubectl apply -f https:&#x2F;&#x2F;docs.projectcalico.org&#x2F;manifests&#x2F;calico.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>After CNI is successfully deployed, we can check the status of the cluster:</p>
<pre class="line-numbers language-none"><code class="language-none">$kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE   IP                NODE                           NOMINATED NODE   READINESS GATES
kube-system   calico-kube-controllers-558995777d-jkkk4   1&#x2F;1     Running   0          24h   192.168.139.67    eighth-md-0-7b848c46d8-cbnwf   &lt;none&gt;           &lt;none&gt;
kube-system   calico-node-6gfdj                          1&#x2F;1     Running   0          24h   10.103.226.236    eighth-md-0-7b848c46d8-7m6h5   &lt;none&gt;           &lt;none&gt;
kube-system   calico-node-92nn8                          1&#x2F;1     Running   0          24h   10.103.226.234    eighth-md-0-7b848c46d8-cbnwf   &lt;none&gt;           &lt;none&gt;
kube-system   calico-node-jbjvx                          1&#x2F;1     Running   0          24h   10.103.226.233    eighth-tf4mg                   &lt;none&gt;           &lt;none&gt;
kube-system   calico-node-k42dc                          1&#x2F;1     Running   0          24h   10.103.226.235    eighth-md-0-7b848c46d8-69m2n   &lt;none&gt;           &lt;none&gt;
kube-system   coredns-74ff55c5b-f9qp7                    1&#x2F;1     Running   0          24h   192.168.139.68    eighth-md-0-7b848c46d8-cbnwf   &lt;none&gt;           &lt;none&gt;
kube-system   coredns-74ff55c5b-qdm4m                    1&#x2F;1     Running   0          24h   192.168.139.66    eighth-md-0-7b848c46d8-cbnwf   &lt;none&gt;           &lt;none&gt;
kube-system   etcd-eighth-tf4mg                          1&#x2F;1     Running   0          24h   10.103.226.233    eighth-tf4mg                   &lt;none&gt;           &lt;none&gt;
kube-system   kube-apiserver-eighth-tf4mg                1&#x2F;1     Running   0          24h   10.103.226.233    eighth-tf4mg                   &lt;none&gt;           &lt;none&gt;
kube-system   kube-controller-manager-eighth-tf4mg       1&#x2F;1     Running   0          24h   10.103.226.233    eighth-tf4mg                   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-2njsn                           1&#x2F;1     Running   0          24h   10.103.226.233    eighth-tf4mg                   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-5cw4z                           1&#x2F;1     Running   0          24h   10.103.226.235    eighth-md-0-7b848c46d8-69m2n   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-5kknb                           1&#x2F;1     Running   0          24h   10.103.226.236    eighth-md-0-7b848c46d8-7m6h5   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-mmtzw                           1&#x2F;1     Running   0          24h   10.103.226.234    eighth-md-0-7b848c46d8-cbnwf   &lt;none&gt;           &lt;none&gt;
kube-system   kube-scheduler-eighth-tf4mg                1&#x2F;1     Running   0          24h   10.103.226.233    eighth-tf4mg                   &lt;none&gt;           &lt;none&gt;
kube-system   kube-vip-eighth-tf4mg                      1&#x2F;1     Running   0          24h   10.103.226.233    eighth-tf4mg                   &lt;none&gt;           &lt;none&gt;
kube-system   vsphere-cloud-controller-manager-2ldmr     1&#x2F;1     Running   0          24h   10.103.226.233    eighth-tf4mg                   &lt;none&gt;           &lt;none&gt;
kube-system   vsphere-cloud-controller-manager-5fp4w     1&#x2F;1     Running   0          24h   10.103.226.235    eighth-md-0-7b848c46d8-69m2n   &lt;none&gt;           &lt;none&gt;
kube-system   vsphere-cloud-controller-manager-6pxj7     1&#x2F;1     Running   0          24h   10.103.226.236    eighth-md-0-7b848c46d8-7m6h5   &lt;none&gt;           &lt;none&gt;
kube-system   vsphere-cloud-controller-manager-nlg8r     1&#x2F;1     Running   0          24h   10.103.226.234    eighth-md-0-7b848c46d8-cbnwf   &lt;none&gt;           &lt;none&gt;
kube-system   vsphere-csi-controller-5456544dd5-9w49q    5&#x2F;5     Running   0          22h   192.168.215.194   eighth-md-0-7b848c46d8-69m2n   &lt;none&gt;           &lt;none&gt;
kube-system   vsphere-csi-node-g4wgk                     3&#x2F;3     Running   0          24h   192.168.139.65    eighth-md-0-7b848c46d8-cbnwf   &lt;none&gt;           &lt;none&gt;
kube-system   vsphere-csi-node-lnt7t                     3&#x2F;3     Running   6          24h   192.168.193.193   eighth-tf4mg                   &lt;none&gt;           &lt;none&gt;
kube-system   vsphere-csi-node-sm2fg                     3&#x2F;3     Running   1          24h   192.168.215.193   eighth-md-0-7b848c46d8-69m2n   &lt;none&gt;           &lt;none&gt;
kube-system   vsphere-csi-node-xcs5d                     3&#x2F;3     Running   1          24h   192.168.250.1     eighth-md-0-7b848c46d8-7m6h5   &lt;none&gt;           &lt;none&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<hr>
<p>All the pods of this cluster are running, which means a Kubernetes cluster is successfully deployed on Vsphere.</p>
<h1 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h1><h2 id="1-No-VM-is-created-on-Vsphere-after-kubectl-apply-f-lt-cluster-yaml-file-gt"><a href="#1-No-VM-is-created-on-Vsphere-after-kubectl-apply-f-lt-cluster-yaml-file-gt" class="headerlink" title="#1 No VM is created on Vsphere after kubectl apply -f &lt;cluster_yaml_file&gt;"></a>#1 No VM is created on Vsphere after <code>kubectl apply -f &lt;cluster_yaml_file&gt;</code></h2><p>NO new VM is created on Vsphere after <code>kubectl apply -f &lt;cluster_yaml_file&gt;</code> command and ControlPlane stuck at <code>WaitingForKubeadmInit:</code></p>
<pre class="line-numbers language-none"><code class="language-none"># Get cluster status
$clusterctl describe cluster test
NAME                                           READY  SEVERITY  REASON                           SINCE  MESSAGE
&#x2F;test                                          False  Info      WaitingForKubeadmInit            109m
├─ClusterInfrastructure - VSphereCluster&#x2F;test  True                                              109m
├─ControlPlane - KubeadmControlPlane&#x2F;test      False  Info      WaitingForKubeadmInit            20h
│ └─Machine&#x2F;test-m7n2j                         True                                              21h
└─Workers
  └─MachineDeployment&#x2F;test-md-0                False  Warning   WaitingForAvailableMachines      21h    Minimum availability requires 3 replicas, current 0 available
    └─3 Machines...                            False  Info      WaitingForControlPlaneAvailable  21h    See test-md-0-f67dc55-dkpqx, test-md-0-f67dc55-g4brk, ...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>We can check the logs of <code>capv-controller-manager</code> and <code>capi-controller-manager</code>:</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment">#capv-controller-manager</span>
$ kubectl logs deploy/capv-controller-manager -f
<span class="token punctuation">..</span>.
E1209 <span class="token number">10</span>:33:47.637562       <span class="token number">1</span> controller.go:317<span class="token punctuation">]</span> controller/vspherevm <span class="token string">"msg"</span><span class="token operator">=</span><span class="token string">"Reconciler error"</span> <span class="token string">"error"</span><span class="token operator">=</span><span class="token string">"failed to reconcile VM: cannot traverse type VirtualMachine"</span> <span class="token string">"name"</span><span class="token operator">=</span><span class="token string">"vsphere-quickstart-sp4j8"</span> <span class="token string">"namespace"</span><span class="token operator">=</span><span class="token string">"default"</span> <span class="token string">"reconciler group"</span><span class="token operator">=</span><span class="token string">"infrastructure.cluster.x-k8s.io"</span> <span class="token string">"reconciler kind"</span><span class="token operator">=</span><span class="token string">"VSphereVM"</span>
I1209 <span class="token number">10</span>:33:59.999674       <span class="token number">1</span> reflector.go:535<span class="token punctuation">]</span> pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167: Watch close - *v1beta1.VSphereMachine total <span class="token number">2</span> items received
I1209 <span class="token number">10</span>:34:12.990178       <span class="token number">1</span> reflector.go:535<span class="token punctuation">]</span> pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167: Watch close - *v1beta1.VSphereDeploymentZone total <span class="token number">0</span> items received

<span class="token comment">#capi-controller-manager</span>
$ kubectl logs deploy/capi-controller-manager -f
I1209 <span class="token number">10</span>:35:44.955263       <span class="token number">1</span> machine_controller_phases.go:282<span class="token punctuation">]</span> controller/machine <span class="token string">"msg"</span><span class="token operator">=</span><span class="token string">"Infrastructure provider is not ready, requeuing"</span> <span class="token string">"cluster"</span><span class="token operator">=</span><span class="token string">"vsphere-quickstart"</span> <span class="token string">"name"</span><span class="token operator">=</span><span class="token string">"vsphere-quickstart-sp4j8"</span> <span class="token string">"namespace"</span><span class="token operator">=</span><span class="token string">"default"</span> <span class="token string">"reconciler group"</span><span class="token operator">=</span><span class="token string">"cluster.x-k8s.io"</span> <span class="token string">"reconciler kind"</span><span class="token operator">=</span><span class="token string">"Machine"</span>
I1209 <span class="token number">10</span>:35:44.955316       <span class="token number">1</span> machine_controller_noderef.go:48<span class="token punctuation">]</span> controller/machine <span class="token string">"msg"</span><span class="token operator">=</span><span class="token string">"Cannot reconcile Machine's Node, no valid ProviderID yet"</span> <span class="token string">"cluster"</span><span class="token operator">=</span><span class="token string">"vsphere-quickstart"</span> <span class="token string">"machine"</span><span class="token operator">=</span><span class="token string">"vsphere-quickstart-sp4j8"</span> <span class="token string">"name"</span><span class="token operator">=</span><span class="token string">"vsphere-quickstart-sp4j8"</span> <span class="token string">"namespace"</span><span class="token operator">=</span><span class="token string">"default"</span> <span class="token string">"reconciler group"</span><span class="token operator">=</span><span class="token string">"cluster.x-k8s.io"</span> <span class="token string">"reconciler kind"</span><span class="token operator">=</span><span class="token string">"Machine"</span>
I1209 <span class="token number">10</span>:36:03.491520       <span class="token number">1</span> machine_controller_phases.go:220<span class="token punctuation">]</span> controller/machine <span class="token string">"msg"</span><span class="token operator">=</span><span class="token string">"Bootstrap provider is not ready, requeuing"</span> <span class="token string">"cluster"</span><span class="token operator">=</span><span class="token string">"vsphere-quickstart"</span> <span class="token string">"name"</span><span class="token operator">=</span><span class="token string">"vsphere-quickstart-md-0-c8d556cb-hfc6g"</span> <span class="token string">"namespace"</span><span class="token operator">=</span><span class="token string">"default"</span> <span class="token string">"reconciler group"</span><span class="token operator">=</span><span class="token string">"cluster.x-k8s.io"</span> <span class="token string">"reconciler kind"</span><span class="token operator">=</span><span class="token string">"Machine"</span>
I1209 <span class="token number">10</span>:36:03.498654       <span class="token number">1</span> machine_controller_phases.go:282<span class="token punctuation">]</span> controller/machine <span class="token string">"msg"</span><span class="token operator">=</span><span class="token string">"Infrastructure provider is not ready, requeuing"</span> <span class="token string">"cluster"</span><span class="token operator">=</span><span class="token string">"vsphere-quickstart"</span> <span class="token string">"name"</span><span class="token operator">=</span><span class="token string">"vsphere-quickstart-md-0-c8d556cb-hfc6g"</span> <span class="token string">"namespace"</span><span class="token operator">=</span><span class="token string">"default"</span> <span class="token string">"reconciler group"</span><span class="token operator">=</span><span class="token string">"cluster.x-k8s.io"</span> <span class="token string">"reconciler kind"</span><span class="token operator">=</span><span class="token string">"Machine"</span>
I1209 <span class="token number">10</span>:36:03.498760       <span class="token number">1</span> machine_controller_noderef.go:48<span class="token punctuation">]</span> controller/machine <span class="token string">"msg"</span><span class="token operator">=</span><span class="token string">"Cannot reconcile Machine's Node, no valid ProviderID yet"</span> <span class="token string">"cluster"</span><span class="token operator">=</span><span class="token string">"vsphere-quickstart"</span> <span class="token string">"machine"</span><span class="token operator">=</span><span class="token string">"vsphere-quickstart-md-0-c8d556cb-hfc6g"</span> <span class="token string">"name"</span><span class="token operator">=</span><span class="token string">"vsphere-quickstart-md-0-c8d556cb-hfc6g"</span> <span class="token string">"namespace"</span><span class="token operator">=</span><span class="token string">"default"</span> <span class="token string">"reconciler group"</span><span class="token operator">=</span><span class="token string">"cluster.x-k8s.io"</span> <span class="token string">"reconciler kind"</span><span class="token operator">=</span><span class="token string">"Machine"</span>
I1209 <span class="token number">10</span>:36:14.966929       <span class="token number">1</span> machine_controller_phases.go:282<span class="token punctuation">]</span> controller/machine <span class="token string">"msg"</span><span class="token operator">=</span><span class="token string">"Infrastructure provider is not ready, requeuing"</span> <span class="token string">"cluster"</span><span class="token operator">=</span><span class="token string">"vsphere-quickstart"</span> <span class="token string">"name"</span><span class="token operator">=</span><span class="token string">"vsphere-quickstart-sp4j8"</span> <span class="token string">"namespace"</span><span class="token operator">=</span><span class="token string">"default"</span> <span class="token string">"reconciler group"</span><span class="token operator">=</span><span class="token string">"cluster.x-k8s.io"</span> <span class="token string">"reconciler kind"</span><span class="token operator">=</span><span class="token string">"Machine"</span>
I1209 <span class="token number">10</span>:36:14.967009       <span class="token number">1</span> machine_controller_noderef.go:48<span class="token punctuation">]</span> controller/machine <span class="token string">"msg"</span><span class="token operator">=</span><span class="token string">"Cannot reconcile Machine's Node, no valid ProviderID yet"</span> <span class="token string">"cluster"</span><span class="token operator">=</span><span class="token string">"vsphere-quickstart"</span> <span class="token string">"machine"</span><span class="token operator">=</span><span class="token string">"vsphere-quickstart-sp4j8"</span> <span class="token string">"name"</span><span class="token operator">=</span><span class="token string">"vsphere-quickstart-sp4j8"</span> <span class="token string">"namespace"</span><span class="token operator">=</span><span class="token string">"default"</span> <span class="token string">"reconciler group"</span><span class="token operator">=</span><span class="token string">"cluster.x-k8s.io"</span> <span class="token string">"reconciler kind"</span><span class="token operator">=</span><span class="token string">"Machine"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>capv-controller-manager</code> printed error message: <code>failed to reconcile VM: cannot traverse type VirtualMachine.</code> It might be because <code>VSPHERE_FOLDER</code> and <code>VSPHERE_RESOURCE_POOL</code> specified in <code>clusterctl.yaml</code> are incorrect.</p>
<p>To get the correct path of <code>VM folder</code> and <code>resource pool</code>, we can use <code>govc CLI</code>( a vSphere command line tool: <a target="_blank" rel="noopener" href="https://github.com/vmware/govmomi/tree/master/govc">https://github.com/vmware/govmomi/tree/master/govc</a>).</p>
<p>(1)VSPHERE_RESOURCE_POOL</p>
<p>For example, if the resource pool’s name is “Test” and we want to get the full path of this resource pool:</p>
<pre class="line-numbers language-none"><code class="language-none">$govc pool.info -dc&#x3D;&lt;data_center_name&gt; Test

Name:               Test
  Path:             &#x2F;&lt;Datacenter&gt;&#x2F;host&#x2F;&lt;host_name&gt;&#x2F;...&#x2F;Resources&#x2F;...&#x2F;Test
  CPU Usage:        965MHz (2.5%)
  CPU Shares:       normal
  CPU Reservation:  0MHz (expandable&#x3D;true)
  CPU Limit:        -1MHz
  Mem Usage:        59816MB (11.7%)
  Mem Shares:       normal
  Mem Reservation:  0MB (expandable&#x3D;true)
  Mem Limit:        -1MB
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>Then the <code>Path</code> shown in the response is the correct path that we can fill in to <code>VSPHERE_RESOURCE_POOL</code> field in <code>clusterctl.yaml</code> file.</p>
<p>(2)VSPHERE_FOLDER</p>
<pre class="line-numbers language-none"><code class="language-none"># govc ls command will list all inventory items in target vsphere:
$govc ls

&#x2F;&lt;datacentor&gt;&#x2F;vm
&#x2F;&lt;datacentor&gt;&#x2F;network
&#x2F;&lt;datacentor&gt;&#x2F;host
&#x2F;&lt;datacentor&gt;&#x2F;datastore

# List items that are stored in &#x2F;&lt;datacentor&gt;&#x2F;vm inventory
$govc ls &#x2F;&lt;datacentor&gt;&#x2F;vm
&#x2F;&lt;datacentor&gt;&#x2F;vm&#x2F;Cluster_Conntroller_10.103.64.218
&#x2F;&lt;datacentor&gt;&#x2F;vm&#x2F;test-win10-lan1
...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<hr>
<p>According to what we’ve got above, it can be told that basically, all the VMs are stored in <code>/&lt;datacenter&gt;/vm</code> inventory, so we can set <code>VSPHERE_FOLDER</code> to “” and vsphere will directly use /<datacenter>/vm as the folder to deploy VM.</p>
<h2 id="2-Stuck-at-WaitingForKubeadmInit-after-VM-is-successfully-provisioned"><a href="#2-Stuck-at-WaitingForKubeadmInit-after-VM-is-successfully-provisioned" class="headerlink" title="#2 Stuck at WaitingForKubeadmInit after VM is successfully provisioned"></a>#2 Stuck at WaitingForKubeadmInit after VM is successfully provisioned</h2><pre class="line-numbers language-none"><code class="language-none">$kubectl get machine
NAME                      CLUSTER   NODENAME   PROVIDERID                                       PHASE         AGE   VERSION
test-m7n2j                test                 vsphere:&#x2F;&#x2F;423aaa89-02b5-4796-7fdc-0b4619a0d4d6   Provisioned   21h   v1.20.1
test-md-0-f67dc55-dkpqx   test                                                                  Pending       21h   v1.20.1
test-md-0-f67dc55-g4brk   test                                                                  Pending       21h   v1.20.1
test-md-0-f67dc55-x48xx   test                                                                  Pending       21h   v1.20.1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<hr>
<p>It can be told from the output we’ve got above that the master node is successfully provisioned (the VM is successfully created on Vsphere).</p>
<p>We can access to target VM using ssh with account <code>capv</code> and <code>ssh key</code> we’ve specified in clusterctl.yaml file.</p>
<pre class="line-numbers language-none"><code class="language-none"># Access to target VM using ssh (Make sure you&#39;ve set VSPHERE_SSH_AUTHORIZED_KEY in cluterctl.yaml)
$ ssh capv@&lt;vm_ip&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>We can check the output of <code>cloud-init</code> first:</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">
<span class="token comment"># check output of cloud-init</span>
$ <span class="token function">cat</span> /var/log/cloud-init-output.log<span class="token operator">|</span><span class="token function">less</span>
<span class="token punctuation">..</span>.
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">17</span>:42:57<span class="token punctuation">]</span> <span class="token punctuation">[</span>kubeconfig<span class="token punctuation">]</span> Using kubeconfig folder <span class="token string">"/etc/kubernetes"</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">17</span>:42:58<span class="token punctuation">]</span> <span class="token punctuation">[</span>kubeconfig<span class="token punctuation">]</span> Writing <span class="token string">"admin.conf"</span> kubeconfig <span class="token function">file</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">17</span>:42:58<span class="token punctuation">]</span> <span class="token punctuation">[</span>kubeconfig<span class="token punctuation">]</span> Writing <span class="token string">"kubelet.conf"</span> kubeconfig <span class="token function">file</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">17</span>:42:58<span class="token punctuation">]</span> <span class="token punctuation">[</span>kubeconfig<span class="token punctuation">]</span> Writing <span class="token string">"controller-manager.conf"</span> kubeconfig <span class="token function">file</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">17</span>:42:58<span class="token punctuation">]</span> <span class="token punctuation">[</span>kubeconfig<span class="token punctuation">]</span> Writing <span class="token string">"scheduler.conf"</span> kubeconfig <span class="token function">file</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">17</span>:42:58<span class="token punctuation">]</span> <span class="token punctuation">[</span>kubelet-start<span class="token punctuation">]</span> Writing kubelet environment <span class="token function">file</span> with flags to <span class="token function">file</span> <span class="token string">"/var/lib/kubelet/kubeadm-flags.env"</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">17</span>:42:58<span class="token punctuation">]</span> <span class="token punctuation">[</span>kubelet-start<span class="token punctuation">]</span> Writing kubelet configuration to <span class="token function">file</span> <span class="token string">"/var/lib/kubelet/config.yaml"</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">17</span>:42:58<span class="token punctuation">]</span> <span class="token punctuation">[</span>kubelet-start<span class="token punctuation">]</span> Starting the kubelet
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">17</span>:42:58<span class="token punctuation">]</span> <span class="token punctuation">[</span>control-plane<span class="token punctuation">]</span> Using manifest folder <span class="token string">"/etc/kubernetes/manifests"</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">17</span>:42:58<span class="token punctuation">]</span> <span class="token punctuation">[</span>control-plane<span class="token punctuation">]</span> Creating static Pod manifest <span class="token keyword">for</span> <span class="token string">"kube-apiserver"</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">17</span>:42:58<span class="token punctuation">]</span> <span class="token punctuation">[</span>control-plane<span class="token punctuation">]</span> Creating static Pod manifest <span class="token keyword">for</span> <span class="token string">"kube-controller-manager"</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">17</span>:42:58<span class="token punctuation">]</span> <span class="token punctuation">[</span>control-plane<span class="token punctuation">]</span> Creating static Pod manifest <span class="token keyword">for</span> <span class="token string">"kube-scheduler"</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">17</span>:42:58<span class="token punctuation">]</span> <span class="token punctuation">[</span>etcd<span class="token punctuation">]</span> Creating static Pod manifest <span class="token keyword">for</span> <span class="token builtin class-name">local</span> etcd <span class="token keyword">in</span> <span class="token string">"/etc/kubernetes/manifests"</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">17</span>:42:58<span class="token punctuation">]</span> <span class="token punctuation">[</span>wait-control-plane<span class="token punctuation">]</span> Waiting <span class="token keyword">for</span> the kubelet to boot up the control plane as static Pods from directory <span class="token string">"/etc/kubernetes/manifests"</span><span class="token builtin class-name">.</span> This can take up to 4m0s
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:09:10<span class="token punctuation">]</span> <span class="token punctuation">[</span>kubelet-check<span class="token punctuation">]</span> Initial <span class="token function">timeout</span> of 40s passed.
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>   Unfortunately, an error has occurred:
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>           timed out waiting <span class="token keyword">for</span> the condition
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>   This error is likely caused by:
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>           - The kubelet is not running
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>           - The kubelet is unhealthy due to a misconfiguration of the node <span class="token keyword">in</span> some way <span class="token punctuation">(</span>required cgroups disabled<span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>   If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>           - <span class="token string">'systemctl status kubelet'</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>           - <span class="token string">'journalctl -xeu kubelet'</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>   Additionally, a control plane component may have crashed or exited when started by the container runtime.
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>   To troubleshoot, list all containers using your preferred container runtimes CLI.
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>   Here is one example how you may list all Kubernetes containers running <span class="token keyword">in</span> cri-o/containerd using crictl:
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>           - <span class="token string">'crictl --runtime-endpoint /var/run/containerd/containerd.sock ps -a | grep kube | grep -v pause'</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>           Once you have found the failing container, you can inspect its logs with:
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>           - <span class="token string">'crictl --runtime-endpoint /var/run/containerd/containerd.sock logs CONTAINERID'</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token number">2021</span>-12-16 <span class="token number">10</span>:12:39<span class="token punctuation">]</span> error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
<span class="token punctuation">..</span>.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>Seems like there was an error occurred while waiting for the kubelet to boot up the control plane. Then we can check the status and logs of <code>kubelet</code>:</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># Check status of kubelet</span>
$ systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded <span class="token punctuation">(</span>/lib/systemd/system/kubelet.service<span class="token punctuation">;</span> enabled<span class="token punctuation">;</span> vendor preset: enabled<span class="token punctuation">)</span>
  Drop-In: /etc/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: active <span class="token punctuation">(</span>running<span class="token punctuation">)</span> since Thu <span class="token number">2021</span>-12-16 <span class="token number">17</span>:42:58 UTC<span class="token punctuation">;</span> 10h ago
     Docs: https://kubernetes.io/docs/home/
 Main PID: <span class="token number">1127</span> <span class="token punctuation">(</span>kubelet<span class="token punctuation">)</span>
    Tasks: <span class="token number">15</span> <span class="token punctuation">(</span>limit: <span class="token number">4915</span><span class="token punctuation">)</span>
   CGroup: /system.slice/kubelet.service
           └─1127 /usr/bin/kubelet --bootstrap-kubeconfig<span class="token operator">=</span>/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig<span class="token operator">=</span>/etc/kubernetes/kubelet.conf --config<span class="token operator">=</span>/var/lib/kubelet/config.yaml --cloud-provider<span class="token operator">=</span>external --container-runtime<span class="token operator">=</span>remote --container-ru

<span class="token comment"># If kubelet is running without any error</span>
<span class="token comment"># use `sudo journalctl -u kubelet` or `sudo journalctl -u kubelet --since &lt;specific_time>`</span>
<span class="token variable">$sudo</span> journalctl -u kubelet --since <span class="token string">"1 day ago"</span><span class="token operator">|</span><span class="token function">less</span>
<span class="token punctuation">..</span>.
Dec <span class="token number">16</span> <span class="token number">10</span>:08:38 fifth-tgq4j kubelet<span class="token punctuation">[</span><span class="token number">1127</span><span class="token punctuation">]</span>: E1216 <span class="token number">10</span>:08:38.885118    <span class="token number">1127</span> remote_image.go:113<span class="token punctuation">]</span> PullImage <span class="token string">"ghcr.io/kube-vip/kube-vip:v0.3.5"</span> from image <span class="token function">service</span> failed: rpc error: code <span class="token operator">=</span> Unknown desc <span class="token operator">=</span> failed to pull and unpack image <span class="token string">"ghcr.io/kube-vip/kube-vip:v0.3.5"</span><span class="token builtin class-name">:</span> failed to resolve reference <span class="token string">"ghcr.io/kube-vip/kube-vip:v0.3.5"</span><span class="token builtin class-name">:</span> failed to <span class="token keyword">do</span> request: Head <span class="token string">"https://ghcr.io/v2/kube-vip/kube-vip/manifests/v0.3.5"</span><span class="token builtin class-name">:</span> dial tcp <span class="token number">20.205</span>.243.164:443: connect: connection refused
Dec <span class="token number">16</span> <span class="token number">10</span>:08:38 fifth-tgq4j kubelet<span class="token punctuation">[</span><span class="token number">1127</span><span class="token punctuation">]</span>: E1216 <span class="token number">10</span>:08:38.885218    <span class="token number">1127</span> kuberuntime_image.go:51<span class="token punctuation">]</span> Pull image <span class="token string">"ghcr.io/kube-vip/kube-vip:v0.3.5"</span> failed: rpc error: code <span class="token operator">=</span> Unknown desc <span class="token operator">=</span> failed to pull and unpack image <span class="token string">"ghcr.io/kube-vip/kube-vip:v0.3.5"</span><span class="token builtin class-name">:</span> failed to resolve reference <span class="token string">"ghcr.io/kube-vip/kube-vip:v0.3.5"</span><span class="token builtin class-name">:</span> failed to <span class="token keyword">do</span> request: Head <span class="token string">"https://ghcr.io/v2/kube-vip/kube-vip/manifests/v0.3.5"</span><span class="token builtin class-name">:</span> dial tcp <span class="token number">20.205</span>.243.164:443: connect: connection refused
Dec <span class="token number">16</span> <span class="token number">10</span>:08:38 fifth-tgq4j kubelet<span class="token punctuation">[</span><span class="token number">1127</span><span class="token punctuation">]</span>: E1216 <span class="token number">10</span>:08:38.885452    <span class="token number">1127</span> kuberuntime_manager.go:829<span class="token punctuation">]</span> container <span class="token operator">&amp;</span>Container<span class="token punctuation">&#123;</span>Name:kube-vip,Image:ghcr.io/kube-vip/kube-vip:v0.3.5,Command:<span class="token punctuation">[</span><span class="token punctuation">]</span>,Args:<span class="token punctuation">[</span>start<span class="token punctuation">]</span>,WorkingDir:,Ports:<span class="token punctuation">[</span><span class="token punctuation">]</span>ContainerPort<span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>,Env:<span class="token punctuation">[</span><span class="token punctuation">]</span>EnvVar<span class="token punctuation">&#123;</span>EnvVar<span class="token punctuation">&#123;</span>Name:vip_arp,Value:true,ValueFrom:nil,<span class="token punctuation">&#125;</span>,EnvVar<span class="token punctuation">&#123;</span>Name:vip_leaderelection,Value:true,ValueFrom:nil,<span class="token punctuation">&#125;</span>,EnvVar<span class="token punctuation">&#123;</span>Name:vip_address,Value:10.103.226.219,ValueFrom:nil,<span class="token punctuation">&#125;</span>,EnvVar<span class="token punctuation">&#123;</span>Name:vip_interface,Value:eth0,ValueFrom:nil,<span class="token punctuation">&#125;</span>,EnvVar<span class="token punctuation">&#123;</span>Name:vip_leaseduration,Value:15,ValueFrom:nil,<span class="token punctuation">&#125;</span>,EnvVar<span class="token punctuation">&#123;</span>Name:vip_renewdeadline,Value:10,ValueFrom:nil,<span class="token punctuation">&#125;</span>,EnvVar<span class="token punctuation">&#123;</span>Name:vip_retryperiod,Value:2,ValueFrom:nil,<span class="token punctuation">&#125;</span>,<span class="token punctuation">&#125;</span>,Resources:ResourceRequirements<span class="token punctuation">&#123;</span>Limits:ResourceList<span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>,Requests:ResourceList<span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>,<span class="token punctuation">&#125;</span>,VolumeMounts:<span class="token punctuation">[</span><span class="token punctuation">]</span>VolumeMount<span class="token punctuation">&#123;</span>VolumeMount<span class="token punctuation">&#123;</span>Name:kubeconfig,ReadOnly:false,MountPath:/etc/kubernetes/admin.conf,SubPath:,MountPropagation:nil,SubPathExpr:,<span class="token punctuation">&#125;</span>,<span class="token punctuation">&#125;</span>,LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:<span class="token operator">&amp;</span>SecurityContext<span class="token punctuation">&#123;</span>Capabilities:<span class="token operator">&amp;</span>Capabilities<span class="token punctuation">&#123;</span>Add:<span class="token punctuation">[</span>NET_ADMIN SYS_TIME<span class="token punctuation">]</span>,Drop:<span class="token punctuation">[</span><span class="token punctuation">]</span>,<span class="token punctuation">&#125;</span>,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,<span class="token punctuation">&#125;</span>,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:<span class="token punctuation">[</span><span class="token punctuation">]</span>EnvFromSource<span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>,TerminationMessagePolicy:File,VolumeDevices:<span class="token punctuation">[</span><span class="token punctuation">]</span>VolumeDevice<span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>,StartupProbe:nil,<span class="token punctuation">&#125;</span> start failed <span class="token keyword">in</span> pod kube-vip-fifth-tgq4j_kube-system<span class="token punctuation">(</span>b31b938f7a5929f365eb5caefef24fa5<span class="token punctuation">)</span>: ErrImagePull: rpc error: code <span class="token operator">=</span> Unknown desc <span class="token operator">=</span> failed to pull and unpack image <span class="token string">"ghcr.io/kube-vip/kube-vip:v0.3.5"</span><span class="token builtin class-name">:</span> failed to resolve reference <span class="token string">"ghcr.io/kube-vip/kube-vip:v0.3.5"</span><span class="token builtin class-name">:</span> failed to <span class="token keyword">do</span> request: Head <span class="token string">"https://ghcr.io/v2/kube-vip/kube-vip/manifests/v0.3.5"</span><span class="token builtin class-name">:</span> dial tcp <span class="token number">20.205</span>.243.164:443: connect: connection refused
Dec <span class="token number">16</span> <span class="token number">10</span>:08:38 fifth-tgq4j kubelet<span class="token punctuation">[</span><span class="token number">1127</span><span class="token punctuation">]</span>: E1216 <span class="token number">10</span>:08:38.885518    <span class="token number">1127</span> pod_workers.go:191<span class="token punctuation">]</span> Error syncing pod b31b938f7a5929f365eb5caefef24fa5 <span class="token punctuation">(</span><span class="token string">"kube-vip-fifth-tgq4j_kube-system(b31b938f7a5929f365eb5caefef24fa5)"</span><span class="token punctuation">)</span>, skipping: failed to <span class="token string">"StartContainer"</span> <span class="token keyword">for</span> <span class="token string">"kube-vip"</span> with ErrImagePull: <span class="token string">"rpc error: code = Unknown desc = failed to pull and unpack image <span class="token entity" title="\&quot;">\"</span>ghcr.io/kube-vip/kube-vip:v0.3.5<span class="token entity" title="\&quot;">\"</span>: failed to resolve reference <span class="token entity" title="\&quot;">\"</span>ghcr.io/kube-vip/kube-vip:v0.3.5<span class="token entity" title="\&quot;">\"</span>: failed to do request: Head <span class="token entity" title="\&quot;">\"</span>https://ghcr.io/v2/kube-vip/kube-vip/manifests/v0.3.5<span class="token entity" title="\&quot;">\"</span>: dial tcp 20.205.243.164:443: connect: connection refused"</span>
Dec <span class="token number">16</span> <span class="token number">10</span>:08:39 fifth-tgq4j kubelet<span class="token punctuation">[</span><span class="token number">1127</span><span class="token punctuation">]</span>: E1216 <span class="token number">10</span>:08:39.268272    <span class="token number">1127</span> pod_workers.go:191<span class="token punctuation">]</span> Error syncing pod b31b938f7a5929f365eb5caefef24fa5 <span class="token punctuation">(</span><span class="token string">"kube-vip-fifth-tgq4j_kube-system(b31b938f7a5929f365eb5caefef24fa5)"</span><span class="token punctuation">)</span>, skipping: failed to <span class="token string">"StartContainer"</span> <span class="token keyword">for</span> <span class="token string">"kube-vip"</span> with ImagePullBackOff: <span class="token string">"Back-off pulling image <span class="token entity" title="\&quot;">\"</span>ghcr.io/kube-vip/kube-vip:v0.3.5<span class="token entity" title="\&quot;">\"</span>"</span>
<span class="token punctuation">..</span>.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>Seems like <code>kudeadm</code> fail to pull image of <code>kube-vip</code> during initialization.</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># Also we can check manifests folder of kubernetes, which contains all</span>
<span class="token comment"># the configurations of static pods that kubernete is going to create during</span>
<span class="token comment"># initailization</span>
$ <span class="token function">ls</span> /etc/kubernetes/manifests
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml  kube-vip.yaml

<span class="token comment"># List all Kubernetes containers running in cri-o/containerd</span>
$ <span class="token function">sudo</span> crictl --runtime-endpoint /var/run/containerd/containerd.sock <span class="token function">ps</span> -a <span class="token operator">|</span> <span class="token function">grep</span> kube <span class="token operator">|</span> <span class="token function">grep</span> -v pause
0154378d48ba2       4aa0b4397bbbc       <span class="token number">20</span> hours ago        Running             kube-scheduler            <span class="token number">0</span>                   ea07bd6eb04ff
57bd6d75bad96       75c7f71120808       <span class="token number">20</span> hours ago        Running             kube-apiserver            <span class="token number">0</span>                   66da2fd70475a
4008afcd0c408       2893d78e47dc3       <span class="token number">20</span> hours ago        Running             kube-controller-manager   <span class="token number">0</span>                   f3b477e5c7ae1
<span class="token punctuation">..</span>.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>If it’s because <code>kudeadm</code> fail to pull the image of <code>kube-vip</code> during initialization, we might need to upload the image manually:</p>
<p>(Assume that the image is successfully downloaded to the local environment)</p>
<pre class="line-numbers language-none"><code class="language-none"># Save docker image to file kube-vip (local environment which might have proxy)
$ docker save ghcr.io&#x2F;kube-vip&#x2F;kube-vip:v0.3.5 -o kube-vip

# Transfer file to target VM
$ scp kube-vip capv@&lt;ip&gt;:~

# Access to target VM using ssh
$ ssh capv@10.103.226.232

# Ask containerd to load image
$ sudo ctr -n k8s.io image import kube-vip<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<hr>
<p>After the image is successfully uploaded to the target VM, we need to reset and run <code>kubeadm init</code> again with the same config. Because kube-proxy and coredns are failed to be initialized since kubeadm init has been interrupted before.</p>
<hr>
<h2 id="3-Kubernetes-pods-stuck-at-ImagePullBackOff-status"><a href="#3-Kubernetes-pods-stuck-at-ImagePullBackOff-status" class="headerlink" title="#3 Kubernetes pods stuck at ImagePullBackOff status"></a>#3 Kubernetes pods stuck at ImagePullBackOff status</h2><p>If <code>ImagePullBackOff</code> error is found when we check the status of the cluster with <code>kubectl get pods --all-namespaces</code> command:</p>
<pre class="line-numbers language-none"><code class="language-none">$ kubectl get pods --all-namespaces
NAMESPACE                           NAME                                                             READY   STATUS             RESTARTS   AGE
capi-kubeadm-bootstrap-system       capi-kubeadm-bootstrap-controller-manager-58945b95bf-nr8kv       0&#x2F;1     ImagePullBackOff   0          3h57m
capi-kubeadm-control-plane-system   capi-kubeadm-control-plane-controller-manager-58fc8f8c7c-jg9wn   0&#x2F;1     ImagePullBackOff   0          3h56m
capi-system                         capi-controller-manager-576744d8b7-vlvjt                         0&#x2F;1     ImagePullBackOff   0          8m7s
capv-system                         capv-controller-manager-6fcb95cd6-pv7m5                          0&#x2F;1     ImagePullBackOff   0          3h28m
cert-manager                        cert-manager-848f547974-gtvzs                                    1&#x2F;1     Running            2          3h57m
...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>This is because the cluster failed to pull images of these components. We can try to set up a proxy to resolve it. </p>
<p>If the proxy doesn’t work, then we need to find another source for these images.</p>
<p>To solve the <code>ImagePullBackOff</code> problem:</p>
<p>(1) Check deployments of these pods and find out which images they need.</p>
<p>(2) Search the images on docker hub and download them.</p>
<p>(3) Use <code>docker tag &lt;origin&gt; &lt;target&gt;</code>  command to change tag of these images.</p>
<p>(4) Use <code>kind load &lt;image tag&gt;</code> command to load the image into the environment of the management cluster.</p>
<p>Then all the pods of the management cluster will run correctly.</p>
<h2 id="4-ControlPlane-stuck-at-PoweringOn-Machine"><a href="#4-ControlPlane-stuck-at-PoweringOn-Machine" class="headerlink" title="#4 ControlPlane stuck at PoweringOn @ Machine/-"></a>#4 ControlPlane stuck at PoweringOn @ Machine/<cluster_name>-<id></h2><pre class="line-numbers language-none"><code class="language-none">$ clusterctl describe cluster &lt;cluster_name&gt;
NAME                                                READY  SEVERITY  REASON                                SINCE  MESSAGE
&#x2F;localt&lt;cluster_name&gt;est                                          False  Info      PoweringOn @ Machine&#x2F;&lt;cluster_name&gt;-svpt4  17h    1 of 2 completed
├─ClusterInfrastructure - VSphereCluster&#x2F;&lt;cluster_name&gt;  True                                                   17h
├─ControlPlane - KubeadmControlPlane&#x2F;&lt;cluster_name&gt;      False  Info      PoweringOn @ Machine&#x2F;&lt;cluster_name&gt;-svpt4  17h    1 of 2 completed
│ └─Machine&#x2F;&lt;cluster_name&gt;-svpt4                         False  Info      PoweringOn                            17h    1 of 2 completed
└─Workers
  └─MachineDeployment&#x2F;&lt;cluster_name&gt;-md-0                False  Warning   WaitingForAvailableMachines           17h    Minimum availability requires 3 replicas, current 0 available
    └─3 Machines...                                 False  Info      WaitingForControlPlaneAvailable       17h    See localtest-md-0-7f754dc455-796h5, localtest-md-0-7f754dc455-7r5kd, ...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<hr>
<p>We can log in to Vsphere center and use the web console to access our new VM(Machine/<cluster_name>-svpt4)</p>
<p>(1) If the <code>VSPHERE_TEMPLATE</code> specified in <code>cluterctl.yaml</code> is centos-<version>, then these messages might be shown in the web console:</p>
<pre class="line-numbers language-none"><code class="language-none">capv.vm kernel: BAR 13 : failed to assign  [io size 0x1000]

capv.vm kernel: BAR 13 : no space for [io size 0x1000]

capv.vm dracut-initqueue[225]: Warning : dracut-initqueue timeout - starting timeout scripts

capv.vm dracut-initqueue[225]: Warning : Could not boot<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>Since <code>cloud-init</code> hasn’t finished initializing, I cannot log in to this VM, thus I haven’t figured out how to deal with this situation. So I used ubuntu instead.</p>
<p>(2) If the VSPHERE_TEMPLATE specified in <code>cluterctl.yaml</code> is ubuntu-<version> , then these messages might be shown in the web console:</p>
<pre class="line-numbers language-none"><code class="language-none">...
failed to start wait for network to be configured
...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<hr>
<p>This might be because DHCP service is not set up in this vsphere center, the created VM should have its own IP address which is assigned by DHCP.</p>
<h2 id="5-Fingerprint-error-certificate-error"><a href="#5-Fingerprint-error-certificate-error" class="headerlink" title="#5 Fingerprint error / certificate error"></a>#5 Fingerprint error / certificate error</h2><p>If a pod of vsphere-cloud-controller-manager stuck at CrashLoopBackOff and its log shows:</p>
<pre class="line-numbers language-none"><code class="language-none">$kubectl logs -n kube-system pod&#x2F;vsphere-csi-controller-5456544dd5-44h77 vsphere-csi-controller
...
&#123;&quot;level&quot;:&quot;error&quot;,&quot;time&quot;:&quot;2021-12-20T11:33:00.238213779Z&quot;,&quot;caller&quot;:&quot;vanilla&#x2F;controller.go:121&quot;,&quot;msg&quot;:&quot;failed to get vcenter. err&#x3D;Post https:&#x2F;&#x2F;...:443&#x2F;sdk: x509: certificate is valid for ..., not ....&quot;,&quot;TraceId&quot;:&quot;d39d14cf-6386-4585-831c-9a5020ab14ab&quot;,...
&#123;&quot;level&quot;:&quot;error&quot;,&quot;time&quot;:&quot;2021-12-20T11:33:00.238251257Z&quot;,&quot;caller&quot;:&quot;service&#x2F;service.go:135&quot;,&quot;msg&quot;:&quot;failed to init controller. Error: Post https:&#x2F;&#x2F;...m:443&#x2F;sdk: x509: certificate is valid for ..., not ...&quot;,&quot;TraceId&quot;:&quot;5e5b850d-4350-4115-8e86-6beb34f2ebad&quot;,...
&#123;&quot;level&quot;:&quot;info&quot;,&quot;time&quot;:&quot;2021-12-20T11:33:00.238357103Z&quot;,&quot;caller&quot;:&quot;service&#x2F;service.go:110&quot;,&quot;msg&quot;:&quot;configured: \&quot;csi.vsphere.vmware.com\&quot; with clusterFlavor: \&quot;VANILLA\&quot; and mode: \&quot;controller\&quot;&quot;,&quot;TraceId&quot;:&quot;5e5b850d-4350-4115-8e86-6beb34f2ebad&quot;&#125;
time&#x3D;&quot;2021-12-20T11:33:00Z&quot; level&#x3D;info msg&#x3D;&quot;removed sock file&quot; path&#x3D;&#x2F;var&#x2F;lib&#x2F;csi&#x2F;sockets&#x2F;pluginproxy&#x2F;csi.sock
time&#x3D;&quot;2021-12-20T11:33:00Z&quot; level&#x3D;fatal msg&#x3D;&quot;grpc failed&quot; error&#x3D;&quot;Post https:&#x2F;&#x2F;&lt;vsphere_server_ip&gt;:443&#x2F;sdk: x509: certificate is valid for ..., not ...&quot;
...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>or</p>
<pre class="line-numbers language-none"><code class="language-none">$kubectl logs -n kube-system pod&#x2F;vsphere-csi-controller-5456544dd5-44h77 vsphere-csi-controller
...
&#123;&quot;level&quot;:&quot;error&quot;,&quot;time&quot;:&quot;2021-12-21T07:03:22.107974392Z&quot;,&quot;caller&quot;:&quot;service&#x2F;service.go:135&quot;,&quot;msg&quot;:&quot;failed to init controller. Error: Post &lt;vsphere_server_ip&gt;:443&#x2F;sdk: x509: certificate signed by unknown authority&quot;...
...
time&#x3D;&quot;2021-12-21T07:03:22Z&quot; level&#x3D;fatal msg&#x3D;&quot;grpc failed&quot; error&#x3D;&quot;Post https:&#x2F;&#x2F;&lt;vsphere_server_ip&gt;:443&#x2F;sdk: x509: certificate signed by unknown authority&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>Please check whether <code>VSPHERE_SERVER</code> field and <code>VSPHERE_TLS_THUMBPRINT</code> in <code>clusterctl.yaml</code> are matched with each other.</p>
<p>If they are matched and you still get an unknown authority error like what is shown above, then you can add <code>insecure-flag = true</code> to <code>secret/csi-vsphere-config</code>:</p>
<pre class="line-numbers language-none"><code class="language-none"># Original secret&#x2F;csi-vsphere-config
apiVersion: v1
data:
  csi-vsphere.conf: W0dsb2JhbF0KY2x1c3Rlci1pZCA9IZhdWx0L2VpZ2h0aCIKaW5z1cmUtZmxhZyA9IGZhbHNlCnRodW1icHJpbnQgPSAiODA6Mjk6N0I6NkU6OUY6MjU6Nzk6QzM6MM6NEY6NDg6Ng6Nzk6N0Q6NEY6NEE6RjI6REE6REQ6MzUiCgpbVmlydHVhbENlbnRlciAidmNlbnRlci...
kind: Secret
metadata:
...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>If you decode the data of csi-vsphere.conf , then you will get:</p>
<pre class="line-numbers language-none"><code class="language-none"># csi-vsphere.conf
[Global]
    cluster-id &#x3D; &quot;default&#x2F;eighth&quot;
...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>Add insecure-flag = true to Global section:</p>
<pre class="line-numbers language-none"><code class="language-none"># csi-vsphere.conf
[Global]
    cluster-id &#x3D; &quot;default&#x2F;eighth&quot;
	insecure-flag &#x3D; true
...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>Update secret/csi-vsphere-config and rerun pod of <code>vsphere-cloud-controller-manager</code>.</p>

  </div>
  <div>
    
      <div 
        class="post-note note-warning copyright" 
        style="margin-top: 42px">
        <p>
          <span style="font-weight: bold;">作者：</span><a 
            target="_blank" 
            rel="nofollow noopener noreferrer" 
            href="/en/about">
            lz-nsc
          </a>
        </p>
        <p>
          <span style="font-weight: bold;">文章链接：</span><a 
            target="_blank" 
            rel="nofollow noopener noreferrer" 
            href="http://lzreload.com/en/2022/09/22/Deploy-cluster-on-Vsphere-using-cluster-api/">
            http://lzreload.com/en/2022/09/22/Deploy-cluster-on-Vsphere-using-cluster-api/
          </a>
        </p>
        <p><span style="font-weight: bold;">版权声明：</span>本博客所有文章除特别声明外，均采用<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0 协议</a>。转载请注明出处！</p>
      </div>
    
  </div>
</article>
<div class="nav">
  
  
    <div class="nav-item-next">
      <a 
        href="/en/2022/09/16/Docker-NetWork-1/" 
        class="nav-link">
        <div>
          <div class="nav-label">Next</div>
          
            <div class="nav-title">About Docker network - Part 1 </div>
          
        </div>
        <i class="iconfont icon-right nav-next-icon"></i>
      </a>
    </div>
  
</div>

  <div 
    class="card card-content comment-card" 
    style="margin-top: 16px;">
    <div class="comment-card-title">评论</div>
    
  <div id="vcomments"></div>
  
  <script>
    loadScript("//unpkg.com/valine/dist/Valine.min.js");
    var oldLoadVa = window.onload;
    window.onload = function () {
      oldLoadVa && oldLoadVa();
      new Valine({
        el: '#vcomments',
        appId: '6AA1gXvoIRhGdQhCpYejA0Us-gzGzoHsz',
        appKey: 'OyxvagUt7NEgTT0JB4WwWIzx',
        placeholder: 'Comment',
        path: window.location.pathname,
        avatar: 'mp',
        meta: ["nick","mail","link"],
        pageSize: '10',
        lang: '',
        visitor: 'false',
        highlight: true,
        recordIP: true,
        
        
        
        enableQQ: 'false',
        requiredFields: [],
      });
    };
  </script>

  </div>

<div 
  class="card card-content toc-card" 
  id="mobiletoc">
  <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>TOC
</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Environment-requirements"><span class="toc-text">Environment requirements</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Deployment-Steps"><span class="toc-text">Deployment Steps</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Configure-clusterctl"><span class="toc-text">1. Configure clusterctl</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Use-Kind-and-Cluserctl-to-create-a-management-cluster"><span class="toc-text">2. Use Kind and Cluserctl to create a management cluster</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Download-the-ova-file-and-deploy-a-template-on-Vsphere"><span class="toc-text">3. Download the ova file and deploy a template on Vsphere</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Deploy-workload-cluster"><span class="toc-text">4. Deploy workload cluster.</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Deploy-CNI"><span class="toc-text">5. Deploy CNI</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Troubleshooting"><span class="toc-text">Troubleshooting</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-No-VM-is-created-on-Vsphere-after-kubectl-apply-f-lt-cluster-yaml-file-gt"><span class="toc-text">#1 No VM is created on Vsphere after kubectl apply -f &lt;cluster_yaml_file&gt;</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Stuck-at-WaitingForKubeadmInit-after-VM-is-successfully-provisioned"><span class="toc-text">#2 Stuck at WaitingForKubeadmInit after VM is successfully provisioned</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Kubernetes-pods-stuck-at-ImagePullBackOff-status"><span class="toc-text">#3 Kubernetes pods stuck at ImagePullBackOff status</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-ControlPlane-stuck-at-PoweringOn-Machine"><span class="toc-text">#4 ControlPlane stuck at PoweringOn @ Machine&#x2F;-</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Fingerprint-error-certificate-error"><span class="toc-text">#5 Fingerprint error &#x2F; certificate error</span></a></li></ol></li></ol>
</div></main>
            <aside class="left-column">
              
              <div class="card card-author">
                
  <img 
    src="/img/author.png" 
    class="author-img"
    width="88"
    height="88"
    alt="author avatar">

<p class="author-name">lz-nsc</p>
<p class="author-description"></p>
<div class="author-message">
  <a 
    class="author-posts-count" 
    href="/en/archives">
    <span>4</span>
    <span>Posts</span>
  </a>
  <a 
    class="author-categories-count" 
    href="/en/categories">
    <span>0</span>
    <span>Categories</span>
  </a>
  <a 
    class="author-tags-count" 
    href="/en/tags">
    <span>5</span>
    <span>Tags</span>
  </a>
</div>

              </div>
               <div class="sticky-tablet">
  
  
    <article class="display-when-two-columns spacer">
      <div class="card card-content toc-card">
        <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>TOC
</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Environment-requirements"><span class="toc-text">Environment requirements</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Deployment-Steps"><span class="toc-text">Deployment Steps</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Configure-clusterctl"><span class="toc-text">1. Configure clusterctl</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Use-Kind-and-Cluserctl-to-create-a-management-cluster"><span class="toc-text">2. Use Kind and Cluserctl to create a management cluster</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Download-the-ova-file-and-deploy-a-template-on-Vsphere"><span class="toc-text">3. Download the ova file and deploy a template on Vsphere</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Deploy-workload-cluster"><span class="toc-text">4. Deploy workload cluster.</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Deploy-CNI"><span class="toc-text">5. Deploy CNI</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Troubleshooting"><span class="toc-text">Troubleshooting</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-No-VM-is-created-on-Vsphere-after-kubectl-apply-f-lt-cluster-yaml-file-gt"><span class="toc-text">#1 No VM is created on Vsphere after kubectl apply -f &lt;cluster_yaml_file&gt;</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Stuck-at-WaitingForKubeadmInit-after-VM-is-successfully-provisioned"><span class="toc-text">#2 Stuck at WaitingForKubeadmInit after VM is successfully provisioned</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Kubernetes-pods-stuck-at-ImagePullBackOff-status"><span class="toc-text">#3 Kubernetes pods stuck at ImagePullBackOff status</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-ControlPlane-stuck-at-PoweringOn-Machine"><span class="toc-text">#4 ControlPlane stuck at PoweringOn @ Machine&#x2F;-</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Fingerprint-error-certificate-error"><span class="toc-text">#5 Fingerprint error &#x2F; certificate error</span></a></li></ol></li></ol>
      </div>
    </article>
  
  
  <article class="card card-content categories-widget">
    <div class="categories-card">
  <div class="categories-header">
    <i 
      class="iconfont icon-fenlei" 
      style="padding-right: 2px;">
    </i>Categories
  </div>
  <div class="categories-list">
    
  </div>
</div>
  </article>
  
  <article class="card card-content tags-widget">
    <div class="tags-card">
  <div class="tags-header">
    <i 
      class="iconfont icon-biaoqian" 
      style="padding-right: 2px;">
    </i>hot tags
  </div>
  <div class="tags-list">
    
      <a 
        href="/en/tags/Kubernetes/" 
        title="Kubernetes">
        <div class="tags-list-item">Kubernetes</div>
      </a>
    
      <a 
        href="/en/tags/Golang/" 
        title="Golang">
        <div class="tags-list-item">Golang</div>
      </a>
    
      <a 
        href="/en/tags/Network/" 
        title="Network">
        <div class="tags-list-item">Network</div>
      </a>
    
      <a 
        href="/en/tags/Docker/" 
        title="Docker">
        <div class="tags-list-item">Docker</div>
      </a>
    
      <a 
        href="/en/tags/Cluster-API/" 
        title="Cluster API">
        <div class="tags-list-item">Cluster API</div>
      </a>
    
  </div>
</div>
  </article>
  
  
</div>
            </aside>
            <aside class="right-column">
              <div class="sticky-widescreen">
  
  
    <article class="card card-content toc-card">
      <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>TOC
</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Environment-requirements"><span class="toc-text">Environment requirements</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Deployment-Steps"><span class="toc-text">Deployment Steps</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Configure-clusterctl"><span class="toc-text">1. Configure clusterctl</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Use-Kind-and-Cluserctl-to-create-a-management-cluster"><span class="toc-text">2. Use Kind and Cluserctl to create a management cluster</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Download-the-ova-file-and-deploy-a-template-on-Vsphere"><span class="toc-text">3. Download the ova file and deploy a template on Vsphere</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Deploy-workload-cluster"><span class="toc-text">4. Deploy workload cluster.</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Deploy-CNI"><span class="toc-text">5. Deploy CNI</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Troubleshooting"><span class="toc-text">Troubleshooting</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-No-VM-is-created-on-Vsphere-after-kubectl-apply-f-lt-cluster-yaml-file-gt"><span class="toc-text">#1 No VM is created on Vsphere after kubectl apply -f &lt;cluster_yaml_file&gt;</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Stuck-at-WaitingForKubeadmInit-after-VM-is-successfully-provisioned"><span class="toc-text">#2 Stuck at WaitingForKubeadmInit after VM is successfully provisioned</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Kubernetes-pods-stuck-at-ImagePullBackOff-status"><span class="toc-text">#3 Kubernetes pods stuck at ImagePullBackOff status</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-ControlPlane-stuck-at-PoweringOn-Machine"><span class="toc-text">#4 ControlPlane stuck at PoweringOn @ Machine&#x2F;-</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Fingerprint-error-certificate-error"><span class="toc-text">#5 Fingerprint error &#x2F; certificate error</span></a></li></ol></li></ol>
    </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header">
    <i 
      class="iconfont icon-wenzhang_huaban" 
      style="padding-right: 2px;">
    </i>Recent Posts
  </div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2022-09-22</div>
        <a href="/en/2022/09/22/Deploy-cluster-on-Vsphere-using-cluster-api/"><div class="recent-posts-item-content">[Cluster API] Deploy cluster on Vsphere</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2022-09-16</div>
        <a href="/en/2022/09/16/Docker-NetWork-1/"><div class="recent-posts-item-content">About Docker network - Part 1</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2022-05-08</div>
        <a href="/en/2022/05/08/About-Kubernetes-configuration-ConfigMap/"><div class="recent-posts-item-content">About Kubernetes configuration: ConfigMap</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2022-04-15</div>
        <a href="/en/2022/04/15/Golang-Heap/"><div class="recent-posts-item-content">Go: About package heap</div></a>
      </div>
    
  </div>
</div>
  </article>
  
  
</div>
            </aside>
          </div>
        </div>
      </div>
    </div>
     
    <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>
          Copyright ©
          
            2022
          
          
        </span>
        &nbsp;
        <a 
          href="/en/" 
          class="footer-link">
          Lynn's blog
        </a>
      </div>
    </div>

    
      <div class="footer-dsc">
        
          Powered by
          <a 
            href="https://hexo.io/" 
            class="footer-link" 
            target="_blank" 
            rel="nofollow noopener noreferrer">
            &nbsp;Hexo
          </a>
        
        
        
      </div>
    
    
    
    
      <div class="footer-dsc">
        
          本站总访问量<span id="busuanzi_value_site_pv"></span>次
        
        
          <span>&nbsp;|&nbsp;</span>
        
        
          本站总访客数<span id="busuanzi_value_site_uv"></span>次
        
      </div>
      
    
</footer>
 
    
  <a 
    role="button" 
    id="scrollbutton" 
    class="basebutton" 
    aria-label="回到顶部">
    <i class="iconfont icon-arrowleft button-icon"></i>
  </a>

<a 
  role="button" 
  id="menubutton"
  aria-label="menu button"
  class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a 
  role="button" 
  id="popbutton" 
  class="basebutton" 
  aria-label="控制中心">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a 
  role="button" 
  id="darkbutton" 
  class="basebutton darkwidget" 
  aria-label="夜色模式">
  <i class="iconfont icon-weather button-icon"></i>
</a>
<a 
  role="button" 
  id="searchbutton" 
  class="basebutton searchwidget" 
  aria-label="搜索">
  <i class="iconfont icon-search button-icon"></i>
</a> 
     
     
     
      <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img')
    var i
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a')
      wrapper.setAttribute('href', img[i].getAttribute('data-src'))
      wrapper.setAttribute('aria-label', 'illustration')
      wrapper.style.cssText =
        'width: 100%; display: flex; justify-content: center;'
      if (img[i].alt) wrapper.dataset.caption = img[i].alt
      wrapper.dataset.nolink = true
      img[i].before(wrapper)
      wrapper.append(img[i])
      var divWrap = document.createElement('div')
      divWrap.classList.add('gallery')
      wrapper.before(divWrap)
      divWrap.append(wrapper)
    }
    baguetteBox.run('.gallery')
  }
</script>
<script>
  loadScript(
    "/en/js/lib/lightbox/baguetteBox.min.js",
    addImgLayout
  )
</script>
 
     
     
    <script src="/en/js/main.js"></script> 
    
      <script> 
        loadScript('/en/js/lib/busuanzi.min.js') 
      </script>
     
    
      <script>
        var addLazyload = function () {
          var observer = lozad('.lozad', {
            load: function (el) {
              el.srcset = el.getAttribute('data-src')
            },
            loaded: function (el) {
              el.classList.add('loaded')
            },
          })
          observer.observe()
        }
      </script>
      <script>
        loadScript('/en/js/lib/lozad.min.js', addLazyload)
      </script>
     
    
    
  </body>
</html>
